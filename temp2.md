### Problem Formulation

在问题的描述部分，作者们定义了假新闻传播检测任务。具体来说，他们使用以下符号和表示法：

- \( D = \{d_1, d_2, ..., d_m\} \) 表示新闻集合， \( m \) 是新闻总数。
- 参与新闻传播的用户集合为 \( U = \{u_1, u_2, ..., u_n\} \)。

对于全局学习，他们构建了一个超图 \( G = (U, E) \) 来描述用户在新闻层面的全局交互， \( E \) 表示超边集合。每个超边 \( e_j \) 连接所有发布或转发第 \( j \) 条新闻 \( d_j \) 的用户。

对于局部学习，作者们定义了新闻 \( D \) 的传播级联和序列，分别为 \( C = \{c_1, c_2, ..., c_m\} \) 和 \( S = \{s_1, s_2, ..., s_m\} \)。其中：
- \( c_j = \{c_{j,1}, c_{j,2}, ..., c_{j,k}\} \) 表示新闻 \( d_j \) 的传播级联，包含一个或多个传播树 \( c_{j,p} = \{(u_i, L_{j,p}^i, I_{j,p}^i) | u_i \in U\} \)， \( L_{j,p}^i \) 和 \( I_{j,p}^i \) 分别表示用户 \( u_i \) 在 \( c_{j,p} \) 中的深度和子节点数，以此保留结构特征。
- \( s_j = \{(u_i, t_j^i) | u_i \in U\} \) 表示新闻 \( d_j \) 的传播序列， \( t_j^i \) 表示 \( u_i \) 传播 \( d_j \) 的时间戳。

每条新闻 \( d_j \) 被分配一个标签 \( y_j \in \{0, 1\} \)，如果新闻 \( d_j \) 是假新闻，则 \( y_j = 1 \)，否则 \( y_j = 0 \)。该工作的任务是通过学习超图 \( G \)、级联 \( c_j \) 和序列 \( s_j \) 来预测 \( d_j \) 的标签。

### The Proposed Model (提出的模型)

在这一部分，作者详细介绍了 HG-SL 模型的整体架构和各个组成部分。HG-SL 模型旨在通过学习用户传播行为来进行假新闻的早期检测。模型的主要组件包括：超图基础的全局交互学习模块和基于自注意力的局部上下文学习模块。

#### 模型整体架构
HG-SL 模型的整体架构如图2所示。不同于直接使用用户属性进行偏好学习（这些属性可能会被伪造身份干扰），HG-SL 模型构建了一个全局传播超图，通过用户的行为模式来更稳健地捕捉他们的偏好。同时，加入节点中心性编码，突出了用户的全局影响力。由于局部传播上下文无法通过图学习获得，模型设计了带有传播状态编码的多头自注意力模块，从结构和时间两个方面学习新闻的局部传播，最后通过门控融合将两个嵌入结合起来，以获得更全面的表达。

#### 1. 超图基础的全局交互学习 (Hypergraph-based Global Interaction Learning)

##### 节点中心性编码 (Node Centrality Encoding)
节点中心性编码用于增强神经网络的学习能力，衡量用户在网络中的全局重要性。在超图中，简单图上的中心性指标（如度中心性和接近中心性）并不适用，因此作者定义了活动度（activity degree）来作为超图中的中心性。活动度定义为节点 \( u_i \) 参与的超边总数：

\[ \text{Act}_i = |E_i| \]

其中，\( E_i \) 是包含节点 \( u_i \) 的超边集合。为了将中心性融入 Hyper-GNN 的训练过程，使用嵌入函数生成中心性向量 \( \text{Cen}_i \)，并直接加到原始嵌入上，得到初始嵌入 \( x_{0i} = x_{in}^i + \text{Cen}_i \)。

##### 超图神经网络 (Hyper-GNN)
超图神经网络通过两阶段聚合来建模用户的全局行为：

###### 节点到超边聚合 (Nodes-to-edge Aggregation)
对于每个超边 \( e_j \)，Hyper-GNN 的第一步是通过聚合其所有连接节点的嵌入来学习其表示 \( a_j \)：

\[ a_j^l = \sigma\left(\sum_{u_i \in e_j} \frac{1}{|e_j|} W_1 x_i^{l-1} \right) \]

其中，\( \sigma \) 是激活函数 ReLU，\( W_1 \in \mathbb{R}^{d_d \times d_d} \) 是可训练的权重矩阵，\( d_d \) 是嵌入的维度，\( l \) 是 Hyper-GNN 的层数。

###### 超边到节点聚合 (Edges-to-node Aggregation)
然后，训练另一个聚合器来整合节点 \( u_i \) 参与的所有超边 \( E_i \)，以更新节点 \( u_i \) 的表示：

\[ x_i^l = \sigma\left(\sum_{e_j \in E_i} \frac{1}{|E_i|} W_2 a_j^l \right) \]

其中，\( \sigma \) 是激活函数 ReLU，\( W_2 \in \mathbb{R}^{d_d \times d_d} \) 是可训练的权重矩阵。经过两阶段聚合后，节点 \( u_i \) 的更新表示不仅包含其自身的信息，还包含与其共享新闻的节点信息，反映其全局偏好。

#### 2. 基于自注意力的局部上下文学习 (Self-attention-based Local Context Learning)

Hyper-GNN 聚焦于新闻和用户的全局关系，但无法描述特定新闻下的内部上下文。因此，作者整合了传播状态编码到两个多头自注意力模块中，从结构和时间两个方面学习新闻的局部表示。

##### 局部时间学习 (Local Temporal Learning)

###### 用户的时间编码 (Temporal encoding of users)
保存每个用户 \( u_i \) 在序列 \( s_j \) 中参与的时间戳 \( t_j^i \)，反映参与者之间的时间差异，并利用嵌入函数生成时间戳向量 \( t_{ji,1} \)。由于时间戳不连续，使用参与的绝对顺序作为位置信息进行自注意力训练，并将其编码为 \( t_{ji,2} \)。上述两个嵌入将直接加到 \( x_i \) 上，获得新闻 \( d_j \) 的时间感知表示：

\[ o_{T}'j = [(x_i + t_{ji,1} + t_{ji,2})|u_i \in s_j] \]

###### 多头自注意力 (Multi-head Self-Attention)
基于自注意力机制在序列任务中的突出表现，应用多头自注意力模块来学习传播的局部上下文。基本学习过程为：

\[ \text{Att}(Q, K, V) = \text{softmax}\left(\frac{Q K'^T}{\sqrt{d_d/H}}\right) V \]

其中 \( H \) 表示注意头的数量，\( K' \) 是 \( K \) 的转置。学习到的表示 \( h_{Tj} \) 计算为：

\[ h_{Tq,j} = \text{Att}(o_{T}'j W_{Qq}^T, o_{T}'j W_{Kq}^T, o_{T}'j W_{Vq}^T) \]
\[ h_{Tj} = [h_{T1,j}; h_{T2,j}; ...; h_{TH,j}] W_{TO} \]

然后使用前馈网络（两层全连接神经网络）获得学习到的序列嵌入，并取平均值作为最终的 \( o_{Tj} \)：

\[ o_{Tj} = \text{MEAN}(W_{A2} \sigma(W_{A1} (h_{Tj})' + b1) + b2) \]

其中 \( \sigma \) 是激活函数 ReLU，\( W_{A1} \) 和 \( W_{A2} \) 是可学习矩阵，\( b1 \) 和 \( b2 \) 是偏置参数。

###### 序列的时间编码 (Temporal encoding of sequence)
由于传播的持续时间 (\( t_{sj,1} \)) 和从推文到转推的平均响应时间 (\( t_{sj,1} \)) 有助于反映新闻 \( d_j \) 的传播速度，作者将上述两个特征作为序列级别的时间特征。由于这些特征是数值浮点型，直接将其与通过自注意力学习的序列表示 (\( o_{Tj} \)) 连接，最终获得时间感知的新闻表示：

\[ Z_{Tj} = [o_{Tj}, t_{sj}] \]

##### 局部结构学习 (Local Structural Learning)
与时间学习类似，训练另一个多头自注意力模块，通过结构编码获得结构感知的局部新闻表示。

###### 用户的结构编码 (Structural encoding of users)
用户 \( u_i \) 在子级联 \( c_{j,p} \) 中引起的转推次数表示用户级别的结构特征，以突出 \( u_i \) 的局部重要性。此外，\( u_i \) 在 \( c_{j,p} \) 中的深度将作为位置信息提供给自注意力学习过程。使用两个嵌入函数从用户重要性和位置生成结构嵌入 \( su_{j,pi,1} \) 和 \( su_{j,pi,2} \)。它们将直接加到嵌入 \( x_i \) 上，获得结构感知的新闻表示：

\[ o_{S}'j = [(x_i + su_{j,pi,1} + su_{j,pi,2})|u_i \in c_{j,p}] \]

###### 级联的结构编码 (Structural encoding of cascades)
由于新闻 \( d_j \) 在传播过程中可能生成多个级联，使用子级联的数量 \( sc_{j,1} \) 和非孤立级联的比例 \( sc_{j,2} \) 表示传播的广度和吸引力，将其与多头自注意力模块学习到的新闻级联表示 (\( o_{Sj} \)) 连接，最终获得结构感知的新闻表示：

\[ Z_{Sj} = [o_{Sj}, sc_{j}] \]

#### 3. 融合与检测 (Fusion & Detection)

##### 门控融合 (Gated Fusion)
为了整合学习到的结构和时间局部传播特征，以获得更具表达力的表示，作者引入了门控融合机制，自适应地结合两个表示：

\[ Z_j = g Z_{Sj} + (1 - g) Z_{Tj} \]

其中，

\[ g = \frac{\exp(W_g \sigma(W_r Z_{Sj}))}{\exp(W_g \sigma(W_r Z_{Tj})) + \exp(W_g \sigma(W_r Z_{Sj}))} \]

##### 假新闻检测 (Fake News Detection)
最终，使用 Softmax 函数计算新闻 \( d_j \) 是否是假新闻的概率：

\[ \hat{y_j} = \text{softmax}(W_p Z_j + b_p) \]

其中，\( W_p \) 是变换矩阵，\( b_p \) 是偏置。通过最小化交叉熵损失进行训练：

\[ J(\theta) = -\frac{1}{m} \left( \sum_{j=1}^m y_j \log(\hat{y_j}) + (1 - y_j) \log(1 - \hat{y_j}) \right) \]

其中，\( \theta \) 表示所有需要学习的参数。如果新闻 \( d_j \) 是假新闻，\( y_j = 1 \)，否则 \( y_j = 0 \)。

这个部分详细描述了 HG-SL 模型的设计和各个组件的功能，以及它们如何共同工作来进行假新闻检测。通过这种方式，模型能够有效捕捉真新闻和假新闻之间的传播模式差异，从而实现假新闻的早期检测。